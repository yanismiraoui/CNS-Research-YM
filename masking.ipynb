{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Learnable Masking Layer\n",
    "class LearnableMaskingLayer(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LearnableMaskingLayer, self).__init__()\n",
    "        self.masking_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.masking_layer(x)\n",
    "        return x * mask\n",
    "\n",
    "# Define the Autoencoder with Learnable Masking\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1024, latent_size=512):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, latent_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.learnable_masking = LearnableMaskingLayer(input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = self.learnable_masking(x)\n",
    "        encoded = self.encoder(x_masked)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3056860864162445\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "autoencoder = MaskedAutoencoder()\n",
    "input_matrix = torch.rand(1, 1024)  # Example input\n",
    "reconstructed = autoencoder(input_matrix)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(reconstructed, input_matrix)  # Compare with the original matrix\n",
    "\n",
    "# Print the loss\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMaskingModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_nodes):\n",
    "        super(GraphMaskingModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, 128)\n",
    "        self.conv2 = GCNConv(128, num_nodes)  # num_nodes is the output dimension\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = torch.sigmoid(self.conv2(x, edge_index))  # Sigmoid for masking\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_nodes = 1024\n",
    "num_features = 3 # Define the number of features per node\n",
    "edges = 4 # Define the edges of your graph\n",
    "\n",
    "# Create a PyG data object\n",
    "data = Data(x=torch.randn(num_nodes, num_features), edge_index=edges)\n",
    "\n",
    "model = GraphMaskingModel(num_features, num_nodes)\n",
    "mask = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectivityMaskingTransformer(nn.Module):\n",
    "    def __init__(self, num_features, seq_length):\n",
    "        super(ConnectivityMaskingTransformer, self).__init__()\n",
    "        config = BertConfig(hidden_size=num_features, num_attention_heads=12, num_hidden_layers=6)\n",
    "        self.transformer = BertModel(config)\n",
    "        self.linear = nn.Linear(num_features, seq_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape (batch_size, seq_length, num_features)\n",
    "        transformer_output = self.transformer(inputs_embeds=x).last_hidden_state\n",
    "        mask = torch.sigmoid(self.linear(transformer_output))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 1024].  Tensor sizes: [1, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rl/d4jq2g217cv1hy_ssxxj4k0r0000gn/T/ipykernel_15551/3181095911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectivityMaskingTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/rl/d4jq2g217cv1hy_ssxxj4k0r0000gn/T/ipykernel_15551/2900170643.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Assuming x is of shape (batch_size, seq_length, num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtransformer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m                 \u001b[0mbuffered_token_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m                 \u001b[0mbuffered_token_type_ids_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffered_token_type_ids_expanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1024) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [1, 1024].  Tensor sizes: [1, 512]"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "seq_length = 1024\n",
    "num_features = 12 # Define the number of features\n",
    "\n",
    "input_seq = torch.randn(1, seq_length, num_features)  # Example input\n",
    "model = ConnectivityMaskingTransformer(num_features, seq_length)\n",
    "mask = model(input_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of converting a numpy array to a PyTorch tensor\n",
    "# Replace this with your actual data loading and preprocessing\n",
    "def load_and_preprocess_data():\n",
    "    # Load your brain connectivity matrices here\n",
    "    # For example, let's create a dummy tensor\n",
    "    data = torch.rand((100, 100))  # 100 matrices of size 100x100\n",
    "    return data\n",
    "\n",
    "class BrainConnectivityTransformer(nn.Module):\n",
    "    def __init__(self, matrix_size, num_layers, num_heads):\n",
    "        super(BrainConnectivityTransformer, self).__init__()\n",
    "        self.matrix_size = matrix_size\n",
    "        self.embedding = nn.Linear(matrix_size, matrix_size)\n",
    "        transformer_layer = nn.TransformerEncoderLayer(d_model=matrix_size, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(matrix_size, matrix_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.matrix_size)  # Reshape to [batch_size*sequence_length, features]\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(-1, 1, self.matrix_size)  # Reshape to [sequence_length, batch_size, features]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.view(-1, self.matrix_size)  # Flatten for the output layer\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.11654290556907654\n",
      "Epoch 2, Loss: 0.07916422188282013\n",
      "Epoch 3, Loss: 0.08018319308757782\n",
      "Epoch 4, Loss: 0.07989799976348877\n",
      "Epoch 5, Loss: 0.0734124407172203\n",
      "Epoch 6, Loss: 0.06462837755680084\n",
      "Epoch 7, Loss: 0.07987625151872635\n",
      "Epoch 8, Loss: 0.05594448000192642\n",
      "Epoch 9, Loss: 0.06626898795366287\n",
      "Epoch 10, Loss: 0.055277567356824875\n"
     ]
    }
   ],
   "source": [
    "def apply_mask(matrix, mask_percentage=0.1):\n",
    "    # Assuming matrix is 2D (sequence length x features)\n",
    "    # Create a mask with the same dimensions as the matrix\n",
    "    mask = torch.rand(matrix.shape) < mask_percentage  # This creates a boolean mask\n",
    "\n",
    "    # Apply the mask to the matrix\n",
    "    masked_matrix = matrix.clone()  # Clone to avoid modifying the original matrix\n",
    "    masked_matrix[mask] = 0  # Set masked elements to zero\n",
    "\n",
    "    return masked_matrix\n",
    "\n",
    "def train_model(model, data, num_epochs):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for matrix in data:\n",
    "            # Reshape matrix to [sequence length, 1, features]\n",
    "            # Assuming each matrix is square with dimensions matrix_size x matrix_size\n",
    "            matrix = matrix.view(-1, model.matrix_size)\n",
    "\n",
    "            # Apply mask\n",
    "            masked_matrix = apply_mask(matrix)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(masked_matrix.view(-1, 1, model.matrix_size))\n",
    "            loss = criterion(output, matrix.view(-1, model.matrix_size))  # Compare with the original matrix\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data = load_and_preprocess_data()\n",
    "model = BrainConnectivityTransformer(matrix_size=100, num_layers=2, num_heads=4)\n",
    "train_model(model, data, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model, matrix, threshold=0.1):\n",
    "    # Assuming matrix is already in the correct shape: [sequence length, 1, features]\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Forward pass through the model\n",
    "        output = model(matrix)\n",
    "\n",
    "    # Calculate the difference between the output and the original matrix\n",
    "    difference = torch.abs(output - matrix)\n",
    "\n",
    "    # Determine the mask based on a threshold\n",
    "    mask = difference > threshold\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ..., False,  True,  True],\n",
       "         [False, False, False,  ..., False,  True,  True],\n",
       "         [False,  True, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False,  True,  True],\n",
       "         [False, False, False,  ..., False,  True,  True],\n",
       "         [False, False, False,  ..., False, False, False]],\n",
       "\n",
       "        [[False, False, False,  ...,  True, False,  True],\n",
       "         [False, False, False,  ..., False, False,  True],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False,  True, False,  ...,  True, False,  True],\n",
       "         [False,  True, False,  ...,  True, False,  True],\n",
       "         [ True, False, False,  ..., False, False, False]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ True, False,  True,  ..., False, False, False],\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ..., False, False, False],\n",
       "         ...,\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [False, False,  True,  ..., False, False, False]],\n",
       "\n",
       "        [[False,  True,  True,  ..., False, False, False],\n",
       "         [False,  True,  True,  ..., False, False, False],\n",
       "         [False,  True, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False,  True,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [ True,  True, False,  ..., False, False, False]],\n",
       "\n",
       "        [[ True, False,  True,  ..., False, False, False],\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [ True,  True,  True,  ...,  True, False, False],\n",
       "         ...,\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [ True, False,  True,  ..., False, False, False],\n",
       "         [ True, False,  True,  ...,  True, False, False]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "new_matrix = torch.rand((100, 100))  # Replace with your new matrix\n",
    "new_matrix = new_matrix.view(-1, 1, 100)  # Reshape to [sequence length, 1, features]\n",
    "\n",
    "mask = predict_mask(model, new_matrix, threshold=0.4)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def matrix_to_graph(matrix):\n",
    "    # Assuming `matrix` is your connectivity matrix\n",
    "\n",
    "    # Create edge index and edge attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] != 0:  # assuming a non-zero value indicates an edge\n",
    "                edge_index.append([i, j])\n",
    "                edge_attr.append(matrix[i, j])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "\n",
    "    # Create a graph data object\n",
    "    data = Data(edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNNMaskingModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim):\n",
    "        super(GNNMaskingModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, 1)  # Output one value per node\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        # Apply GNN layers\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_attr))\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, graph_data, num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.MSELoss()  # or any other suitable loss function\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data)\n",
    "        loss = criterion(out, graph_data.y)  # Compare with target values\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(model, graph_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph_data)\n",
    "\n",
    "    # Assuming that higher values indicate more importance\n",
    "    mask = out > some_threshold  # Define some_threshold based on your requirements\n",
    "\n",
    "    return mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
